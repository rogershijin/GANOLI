{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7434fbb5-53de-4dd6-af38-0160591b53f6",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "- try converting these to .tocsr().tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192814ff-24d8-4917-9956-6fd35ce50543",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ddf764-80bb-4e66-9079-0627fdc49793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import os\n",
    "remote_atac_dir = '/om2/user/rogerjin/data/NeurIPS2021/multiome/atac'\n",
    "remote_rna_dir = '/om2/user/rogerjin/data/NeurIPS2021/multiome/rna'\n",
    "os.makedirs(remote_rna_dir, exist_ok=True)\n",
    "remote_atac_path = '/om2/user/rogerjin/data/NeurIPS2021/multiome/multiome_atac_processed_training.h5ad'\n",
    "remote_rna_path = '/om2/user/rogerjin/data/NeurIPS2021/multiome/multiome_gex_processed_training.h5ad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "33946c5d-9828-4ffc-8804-4f5ae2e9cc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 42492 × 116490\n",
       "    obs: 'nCount_peaks', 'atac_fragments', 'reads_in_peaks_frac', 'blacklist_fraction', 'nucleosome_signal', 'cell_type', 'pseudotime_order_ATAC', 'batch', 'pseudotime_order_GEX', 'is_train'\n",
       "    var: 'feature_types'\n",
       "    uns: 'dataset_id', 'gene_activity_var_names', 'organism', 'sample_pm_varnames'\n",
       "    obsm: 'gene_activity', 'lsi_full', 'lsi_red', 'umap'\n",
       "    layers: 'counts'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 42492 × 13431\n",
       "    obs: 'pct_counts_mt', 'n_counts', 'n_genes', 'size_factors', 'phase', 'cell_type', 'pseudotime_order_GEX', 'batch', 'pseudotime_order_ATAC', 'is_train'\n",
       "    var: 'gene_ids', 'feature_types', 'genome'\n",
       "    uns: 'dataset_id', 'organism'\n",
       "    obsm: 'X_pca', 'X_umap'\n",
       "    layers: 'counts'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "atac = sc.read_h5ad(remote_atac_path)\n",
    "rna = sc.read_h5ad(remote_rna_path)\n",
    "display(atac)\n",
    "display(rna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6625f6b8-285c-419a-94b9-08b5227ad3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6d15587-795b-4278-99db-0956e676d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_train, atac_val_test, rna_train, rna_val_test = train_test_split(atac, rna, test_size=0.2, random_state=42)\n",
    "atac_val, atac_test, rna_val, rna_test = train_test_split(atac_val_test, rna_val_test, test_size=0.5, random_state=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3f1213cc-4975-40f6-874a-cdb70bc7f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_split = {\n",
    "    'train': atac_train,\n",
    "    'val': atac_val,\n",
    "    'test': atac_test\n",
    "}\n",
    "\n",
    "rna_split = {\n",
    "    'train': rna_train,\n",
    "    'val': rna_val,\n",
    "    'test': rna_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7a6e2c9-efaa-4d14-989f-070083b31db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, split in atac_split.items():\n",
    "    split.write_h5ad(f'{remote_atac_dir}/atac_{split_name}.h5ad')\n",
    "    \n",
    "for split_name, split in rna_split.items():\n",
    "    split.write_h5ad(f'{remote_rna_dir}/rna_{split_name}.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45e26d9-957b-4030-9fd4-fe4a048d80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_split = {\n",
    "    'train': sc.read_h5ad(f'{remote_atac_dir}/atac_train.h5ad'),\n",
    "    'val': sc.read_h5ad(f'{remote_atac_dir}/atac_val.h5ad'),\n",
    "    'test': sc.read_h5ad(f'{remote_atac_dir}/atac_test.h5ad')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1f3109-8f68-47bd-8524-13cffb6a3274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.00756116, 0.00857404, 0.01617414, ..., 0.00712643, 0.14550444,\n",
       "         0.01257566]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def var(a, axis=None):\n",
    "    \"\"\" Variance of sparse matrix a\n",
    "    var = mean(a**2) - mean(a)**2\n",
    "    source: https://gist.github.com/sumartoyo/edba2eee645457a98fdf046e1b4297e4\n",
    "    \"\"\"\n",
    "    a_squared = a.copy()\n",
    "    a_squared.data **= 2\n",
    "    return a_squared.mean(axis) - np.square(a.mean(axis))\n",
    "\n",
    "var_atac_train = var(atac_split['train'].X, axis=0)\n",
    "var_atac_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cbc1847-35a2-4989-93db-d3a6eb93b48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.24999275, 0.24982592, 0.24817671, ..., 0.0014394 , 0.00141006,\n",
       "         0.00138073]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_indices = np.fliplr(var_atac_train.argsort())\n",
    "sorted_indices = np.asarray(sorted_indices).squeeze()\n",
    "display(var_atac_train[:, sorted_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b599ffe3-3c9a-47bd-b34b-e7e4de5b26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(sorted_indices.copy()), f'{remote_atac_dir}/sorted_indices_decreasing_variance.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a96e7ca-a91c-4649-bf4a-dd03f5714a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, split in atac_split.items():\n",
    "    split[:, sorted_indices].write_h5ad(f'{remote_atac_dir}/atac_{split_name}_sorted_decreasing_variance.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa4da7d-f669-4b1b-be11-149b80bb2ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_512 = sorted_indices.tolist()[:512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a4fb8-89b0-4785-aaa0-d176977737af",
   "metadata": {},
   "source": [
    "Not sure if this inbalance matters or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1cc18cc-ae6b-4ff5-8f3b-087f42e55362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.histplot(top_512)\n",
    "_ = ax.set(xlabel='Peak index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29393f-3d00-4945-addb-903859332d62",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa6331f-1b9b-4cd8-a7ae-f1583355014e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrogershijin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/kellislab/rogerjin/GANOLI/wandb/run-20220902_211105-3m0zjo9c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rogershijin/Squish%20Transformer/runs/3m0zjo9c\" target=\"_blank\">cosmic-snowflake-8</a></strong> to <a href=\"https://wandb.ai/rogershijin/Squish%20Transformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rogershijin/Squish%20Transformer/runs/3m0zjo9c?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b751ab4f9d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "config = {\n",
    "    'batch_size': 8,\n",
    "    'lr': 5e-4,\n",
    "}\n",
    "\n",
    "wandb.init(project=\"Squish Transformer\", entity=\"rogershijin\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0f13cd-750f-4af8-a0f8-33d5224747f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "atac = {\n",
    "    'train': sc.read_h5ad(f'{remote_atac_dir}/atac_train_sorted_decreasing_variance.h5ad'),\n",
    "    'val': sc.read_h5ad(f'{remote_atac_dir}/atac_val_sorted_decreasing_variance.h5ad'),\n",
    "    'test': sc.read_h5ad(f'{remote_atac_dir}/atac_test_sorted_decreasing_variance.h5ad')\n",
    "}\n",
    "\n",
    "rna = {\n",
    "    'train': sc.read_h5ad(f'{remote_rna_dir}/rna_train.h5ad'),\n",
    "    'val': sc.read_h5ad(f'{remote_rna_dir}/rna_val.h5ad'),\n",
    "    'test': sc.read_h5ad(f'{remote_rna_dir}/rna_test.h5ad')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc2d280-6052-4beb-899f-7a1627294cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': MuData object with n_obs × n_vars = 33993 × 129921\n",
       "   var:\t'feature_types'\n",
       "   2 modalities\n",
       "     atac:\t33993 x 116490\n",
       "       obs:\t'nCount_peaks', 'atac_fragments', 'reads_in_peaks_frac', 'blacklist_fraction', 'nucleosome_signal', 'cell_type', 'pseudotime_order_ATAC', 'batch', 'pseudotime_order_GEX', 'is_train'\n",
       "       var:\t'feature_types'\n",
       "       uns:\t'dataset_id', 'gene_activity_var_names', 'organism', 'sample_pm_varnames'\n",
       "       obsm:\t'gene_activity', 'lsi_full', 'lsi_red', 'umap'\n",
       "       layers:\t'counts'\n",
       "     rna:\t33993 x 13431\n",
       "       obs:\t'pct_counts_mt', 'n_counts', 'n_genes', 'size_factors', 'phase', 'cell_type', 'pseudotime_order_GEX', 'batch', 'pseudotime_order_ATAC', 'is_train'\n",
       "       var:\t'gene_ids', 'feature_types', 'genome'\n",
       "       uns:\t'dataset_id', 'organism'\n",
       "       obsm:\t'X_pca', 'X_umap'\n",
       "       layers:\t'counts',\n",
       " 'val': MuData object with n_obs × n_vars = 4249 × 129921\n",
       "   var:\t'feature_types'\n",
       "   2 modalities\n",
       "     atac:\t4249 x 116490\n",
       "       obs:\t'nCount_peaks', 'atac_fragments', 'reads_in_peaks_frac', 'blacklist_fraction', 'nucleosome_signal', 'cell_type', 'pseudotime_order_ATAC', 'batch', 'pseudotime_order_GEX', 'is_train'\n",
       "       var:\t'feature_types'\n",
       "       uns:\t'dataset_id', 'gene_activity_var_names', 'organism', 'sample_pm_varnames'\n",
       "       obsm:\t'gene_activity', 'lsi_full', 'lsi_red', 'umap'\n",
       "       layers:\t'counts'\n",
       "     rna:\t4249 x 13431\n",
       "       obs:\t'pct_counts_mt', 'n_counts', 'n_genes', 'size_factors', 'phase', 'cell_type', 'pseudotime_order_GEX', 'batch', 'pseudotime_order_ATAC', 'is_train'\n",
       "       var:\t'gene_ids', 'feature_types', 'genome'\n",
       "       uns:\t'dataset_id', 'organism'\n",
       "       obsm:\t'X_pca', 'X_umap'\n",
       "       layers:\t'counts',\n",
       " 'test': MuData object with n_obs × n_vars = 4250 × 129921\n",
       "   var:\t'feature_types'\n",
       "   2 modalities\n",
       "     atac:\t4250 x 116490\n",
       "       obs:\t'nCount_peaks', 'atac_fragments', 'reads_in_peaks_frac', 'blacklist_fraction', 'nucleosome_signal', 'cell_type', 'pseudotime_order_ATAC', 'batch', 'pseudotime_order_GEX', 'is_train'\n",
       "       var:\t'feature_types'\n",
       "       uns:\t'dataset_id', 'gene_activity_var_names', 'organism', 'sample_pm_varnames'\n",
       "       obsm:\t'gene_activity', 'lsi_full', 'lsi_red', 'umap'\n",
       "       layers:\t'counts'\n",
       "     rna:\t4250 x 13431\n",
       "       obs:\t'pct_counts_mt', 'n_counts', 'n_genes', 'size_factors', 'phase', 'cell_type', 'pseudotime_order_GEX', 'batch', 'pseudotime_order_ATAC', 'is_train'\n",
       "       var:\t'gene_ids', 'feature_types', 'genome'\n",
       "       uns:\t'dataset_id', 'organism'\n",
       "       obsm:\t'X_pca', 'X_umap'\n",
       "       layers:\t'counts'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ganoli.GanoliDataset import GanoliMultimodalDataset\n",
    "from muon import MuData\n",
    "\n",
    "class MuDataWithLen(MuData):\n",
    "    \n",
    "    def __len__(self):\n",
    "        try:\n",
    "            return self._len\n",
    "        except:\n",
    "            self._len = min(len(mod) for mod in self.mod.values())\n",
    "            return self._len\n",
    "\n",
    "datasets = {\n",
    "    partition: MuDataWithLen({'atac': atac[partition], 'rna': rna[partition]}) for partition in atac.keys()\n",
    "}\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "749c372b-e2c4-46da-bbea-41e8036700c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from muon import MuData as md\n",
    "from torch.utils.data import DataLoader, BatchSampler, SequentialSampler, RandomSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "samplers = {\n",
    "    'train': RandomSampler,\n",
    "    'val': SequentialSampler,\n",
    "    'test': SequentialSampler\n",
    "}\n",
    "\n",
    "loaders = {\n",
    "    partition: DataLoader(dataset, sampler=BatchSampler(samplers[partition](dataset), batch_size=BATCH_SIZE, drop_last=False ), collate_fn=lambda x: x[0]) for partition, dataset in datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a7bc05-290a-4e55-a479-613b67763e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote\n"
     ]
    }
   ],
   "source": [
    "LOCAL = False\n",
    "cache_dir=None if LOCAL else \"/om2/user/rogerjin/.cache\"\n",
    "print(\"local\" if LOCAL else \"remote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5948702a-ada2-4cb1-a9e1-88200b970ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del squished\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del rna\n",
    "except:\n",
    "    pass\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f0c68258-0ce3-4d55-b725-b255ede328e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "from torch.nn import Linear\n",
    "\n",
    "class SquishTransformer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, output_dim=13431):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased', cache_dir=cache_dir)\n",
    "        self.distilbert.embeddings.word_embeddings = torch.nn.Embedding(116491, 768) # todo: magic numbers\n",
    "        self.pre_classifier = Linear(self.distilbert.config.dim, self.distilbert.config.dim)\n",
    "        self.classifier = Linear(self.distilbert.config.dim, output_dim)\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        out = self.distilbert(**kwargs).last_hidden_state[:, 0] # embedding of cls\n",
    "        out = self.pre_classifier(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "model = SquishTransformer()\n",
    "# device = 'cpu'\n",
    "device = 'cuda:0'\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8337119e-11db-4180-930a-9e74268b476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forout torch.Size([1, 768])\n",
      "END OF EPOCH 1\n",
      "Loss=0.17400246858596802\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.68 GiB (GPU 0; 79.35 GiB total capacity; 71.24 GiB already allocated; 259.19 MiB free; 77.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-4eed16f90bec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msquished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquish_and_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistilbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mrna\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rna'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msquished\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msquished\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om2/user/rogerjin/conda/ganoli/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-2ebdb5e7024b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistilbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# embedding of cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'forout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om2/user/rogerjin/conda/ganoli/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om2/user/rogerjin/conda/ganoli/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         )\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om2/user/rogerjin/conda/ganoli/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om2/user/rogerjin/conda/ganoli/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             )\n\u001b[1;32m    352\u001b[0m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om2/user/rogerjin/conda/ganoli/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om2/user/rogerjin/conda/ganoli/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         )\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om2/user/rogerjin/conda/ganoli/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/om2/user/rogerjin/conda/ganoli/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_reshp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, k_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.68 GiB (GPU 0; 79.35 GiB total capacity; 71.24 GiB already allocated; 259.19 MiB free; 77.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from squish_indexing import squish_and_embed\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "device = 'cuda:0'\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch in loaders['train']:\n",
    "        optimizer.zero_grad()\n",
    "        atac = batch.mod['atac'].layers['counts'].tocsr().tocoo()\n",
    "        squished = squish_and_embed(atac, model.distilbert.embeddings.word_embeddings)\n",
    "        rna = torch.tensor(batch.mod['rna'].X.todense()).float().to(device)\n",
    "        out = model(inputs_embeds=squished['embeddings'], attention_mask=squished['attention_mask'])\n",
    "        loss = loss_fn(out, rna)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'END OF EPOCH {epoch+1}')\n",
    "    print(f'Loss={total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28cec5af-930b-401b-810b-99479f412b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [1,2,3,4]\n",
    "t = torch.tensor(A)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e1f1f4-5733-49b9-a7f5-bc38065b1375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/om2/user/rogerjin/conda/ganoli/bin/python'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ganoli",
   "language": "python",
   "name": "ganoli"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
