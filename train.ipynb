{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7434fbb5-53de-4dd6-af38-0160591b53f6",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "- try converting these to .tocsr().tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192814ff-24d8-4917-9956-6fd35ce50543",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ddf764-80bb-4e66-9079-0627fdc49793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import os\n",
    "remote_atac_dir = '/om2/user/rogerjin/data/NeurIPS2021/multiome/atac'\n",
    "remote_rna_dir = '/om2/user/rogerjin/data/NeurIPS2021/multiome/rna'\n",
    "os.makedirs(remote_rna_dir, exist_ok=True)\n",
    "remote_atac_path = '/om2/user/rogerjin/data/NeurIPS2021/multiome/multiome_atac_processed_training.h5ad'\n",
    "remote_rna_path = '/om2/user/rogerjin/data/NeurIPS2021/multiome/multiome_gex_processed_training.h5ad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "33946c5d-9828-4ffc-8804-4f5ae2e9cc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 42492 × 116490\n",
       "    obs: 'nCount_peaks', 'atac_fragments', 'reads_in_peaks_frac', 'blacklist_fraction', 'nucleosome_signal', 'cell_type', 'pseudotime_order_ATAC', 'batch', 'pseudotime_order_GEX', 'is_train'\n",
       "    var: 'feature_types'\n",
       "    uns: 'dataset_id', 'gene_activity_var_names', 'organism', 'sample_pm_varnames'\n",
       "    obsm: 'gene_activity', 'lsi_full', 'lsi_red', 'umap'\n",
       "    layers: 'counts'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 42492 × 13431\n",
       "    obs: 'pct_counts_mt', 'n_counts', 'n_genes', 'size_factors', 'phase', 'cell_type', 'pseudotime_order_GEX', 'batch', 'pseudotime_order_ATAC', 'is_train'\n",
       "    var: 'gene_ids', 'feature_types', 'genome'\n",
       "    uns: 'dataset_id', 'organism'\n",
       "    obsm: 'X_pca', 'X_umap'\n",
       "    layers: 'counts'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "atac = sc.read_h5ad(remote_atac_path)\n",
    "rna = sc.read_h5ad(remote_rna_path)\n",
    "display(atac)\n",
    "display(rna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6625f6b8-285c-419a-94b9-08b5227ad3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6d15587-795b-4278-99db-0956e676d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_train, atac_val_test, rna_train, rna_val_test = train_test_split(atac, rna, test_size=0.2, random_state=42)\n",
    "atac_val, atac_test, rna_val, rna_test = train_test_split(atac_val_test, rna_val_test, test_size=0.5, random_state=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3f1213cc-4975-40f6-874a-cdb70bc7f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_split = {\n",
    "    'train': atac_train,\n",
    "    'val': atac_val,\n",
    "    'test': atac_test\n",
    "}\n",
    "\n",
    "rna_split = {\n",
    "    'train': rna_train,\n",
    "    'val': rna_val,\n",
    "    'test': rna_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7a6e2c9-efaa-4d14-989f-070083b31db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, split in atac_split.items():\n",
    "    split.write_h5ad(f'{remote_atac_dir}/atac_{split_name}.h5ad')\n",
    "    \n",
    "for split_name, split in rna_split.items():\n",
    "    split.write_h5ad(f'{remote_rna_dir}/rna_{split_name}.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45e26d9-957b-4030-9fd4-fe4a048d80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_split = {\n",
    "    'train': sc.read_h5ad(f'{remote_atac_dir}/atac_train.h5ad'),\n",
    "    'val': sc.read_h5ad(f'{remote_atac_dir}/atac_val.h5ad'),\n",
    "    'test': sc.read_h5ad(f'{remote_atac_dir}/atac_test.h5ad')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1f3109-8f68-47bd-8524-13cffb6a3274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.00756116, 0.00857404, 0.01617414, ..., 0.00712643, 0.14550444,\n",
       "         0.01257566]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def var(a, axis=None):\n",
    "    \"\"\" Variance of sparse matrix a\n",
    "    var = mean(a**2) - mean(a)**2\n",
    "    source: https://gist.github.com/sumartoyo/edba2eee645457a98fdf046e1b4297e4\n",
    "    \"\"\"\n",
    "    a_squared = a.copy()\n",
    "    a_squared.data **= 2\n",
    "    return a_squared.mean(axis) - np.square(a.mean(axis))\n",
    "\n",
    "var_atac_train = var(atac_split['train'].X, axis=0)\n",
    "var_atac_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cbc1847-35a2-4989-93db-d3a6eb93b48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.24999275, 0.24982592, 0.24817671, ..., 0.0014394 , 0.00141006,\n",
       "         0.00138073]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_indices = np.fliplr(var_atac_train.argsort())\n",
    "sorted_indices = np.asarray(sorted_indices).squeeze()\n",
    "display(var_atac_train[:, sorted_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b599ffe3-3c9a-47bd-b34b-e7e4de5b26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.tensor(sorted_indices.copy()), f'{remote_atac_dir}/sorted_indices_decreasing_variance.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a96e7ca-a91c-4649-bf4a-dd03f5714a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, split in atac_split.items():\n",
    "    split[:, sorted_indices].write_h5ad(f'{remote_atac_dir}/atac_{split_name}_sorted_decreasing_variance.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa4da7d-f669-4b1b-be11-149b80bb2ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_512 = sorted_indices.tolist()[:512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a4fb8-89b0-4785-aaa0-d176977737af",
   "metadata": {},
   "source": [
    "Not sure if this inbalance matters or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1cc18cc-ae6b-4ff5-8f3b-087f42e55362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.histplot(top_512)\n",
    "_ = ax.set(xlabel='Peak index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29393f-3d00-4945-addb-903859332d62",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f7b821f-c877-4b85-a823-e1ef9d5d5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "config = {\n",
    "    'run_name': 'test-run',\n",
    "    'batch_size': 32,\n",
    "    'lr': 5e-4,\n",
    "    'max_seq_len': 1600,\n",
    "    'epochs': 1000,\n",
    "    'use_binary': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67675b0b-f6a1-4c5e-a81d-09ad1591b4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b92931f99af43a2b8d5c12b00b93514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">test-run</strong>: <a href=\"https://wandb.ai/rogershijin/Squish%20Transformer/runs/g10yis5d\" target=\"_blank\">https://wandb.ai/rogershijin/Squish%20Transformer/runs/g10yis5d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220905_140418-g10yis5d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/net/vast-storage/scratch/vast/kellislab/rogerjin/GANOLI/wandb/run-20220905_140546-2csoiizd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/rogershijin/Squish%20Transformer/runs/2csoiizd\" target=\"_blank\">test-run</a></strong> to <a href=\"https://wandb.ai/rogershijin/Squish%20Transformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'batch_size': 32,\n",
      "  'checkpoint_dir': '/om2/user/rogerjin/checkpoints/test-run',\n",
      "  'epochs': 1000,\n",
      "  'lr': 0.0005,\n",
      "  'max_seq_len': 1600,\n",
      "  'run_name': 'test-run',\n",
      "  'use_binary': True}\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "try:\n",
    "    wandb.finish()\n",
    "except:\n",
    "    pass\n",
    "wandb.init(project=\"Squish Transformer\", entity=\"rogershijin\", reinit=True, config=config, name=config.get('run_name', None))\n",
    "checkpoint_dir = f'/om2/user/rogerjin/checkpoints/{wandb.run.name}'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "wandb.config.update({'checkpoint_dir': checkpoint_dir})\n",
    "pprint.pprint(dict(wandb.config), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0f13cd-750f-4af8-a0f8-33d5224747f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "atac = {\n",
    "    'train': sc.read_h5ad(f'{remote_atac_dir}/atac_train_sorted_decreasing_variance.h5ad'),\n",
    "    'val': sc.read_h5ad(f'{remote_atac_dir}/atac_val_sorted_decreasing_variance.h5ad'),\n",
    "    'test': sc.read_h5ad(f'{remote_atac_dir}/atac_test_sorted_decreasing_variance.h5ad')\n",
    "}\n",
    "\n",
    "rna = {\n",
    "    'train': sc.read_h5ad(f'{remote_rna_dir}/rna_train.h5ad'),\n",
    "    'val': sc.read_h5ad(f'{remote_rna_dir}/rna_val.h5ad'),\n",
    "    'test': sc.read_h5ad(f'{remote_rna_dir}/rna_test.h5ad')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc2d280-6052-4beb-899f-7a1627294cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': MuData object with n_obs × n_vars = 33993 × 129921\n",
       "   var:\t'feature_types'\n",
       "   2 modalities\n",
       "     atac:\t33993 x 116490\n",
       "       obs:\t'nCount_peaks', 'atac_fragments', 'reads_in_peaks_frac', 'blacklist_fraction', 'nucleosome_signal', 'cell_type', 'pseudotime_order_ATAC', 'batch', 'pseudotime_order_GEX', 'is_train'\n",
       "       var:\t'feature_types'\n",
       "       uns:\t'dataset_id', 'gene_activity_var_names', 'organism', 'sample_pm_varnames'\n",
       "       obsm:\t'gene_activity', 'lsi_full', 'lsi_red', 'umap'\n",
       "       layers:\t'counts'\n",
       "     rna:\t33993 x 13431\n",
       "       obs:\t'pct_counts_mt', 'n_counts', 'n_genes', 'size_factors', 'phase', 'cell_type', 'pseudotime_order_GEX', 'batch', 'pseudotime_order_ATAC', 'is_train'\n",
       "       var:\t'gene_ids', 'feature_types', 'genome'\n",
       "       uns:\t'dataset_id', 'organism'\n",
       "       obsm:\t'X_pca', 'X_umap'\n",
       "       layers:\t'counts',\n",
       " 'val': MuData object with n_obs × n_vars = 4249 × 129921\n",
       "   var:\t'feature_types'\n",
       "   2 modalities\n",
       "     atac:\t4249 x 116490\n",
       "       obs:\t'nCount_peaks', 'atac_fragments', 'reads_in_peaks_frac', 'blacklist_fraction', 'nucleosome_signal', 'cell_type', 'pseudotime_order_ATAC', 'batch', 'pseudotime_order_GEX', 'is_train'\n",
       "       var:\t'feature_types'\n",
       "       uns:\t'dataset_id', 'gene_activity_var_names', 'organism', 'sample_pm_varnames'\n",
       "       obsm:\t'gene_activity', 'lsi_full', 'lsi_red', 'umap'\n",
       "       layers:\t'counts'\n",
       "     rna:\t4249 x 13431\n",
       "       obs:\t'pct_counts_mt', 'n_counts', 'n_genes', 'size_factors', 'phase', 'cell_type', 'pseudotime_order_GEX', 'batch', 'pseudotime_order_ATAC', 'is_train'\n",
       "       var:\t'gene_ids', 'feature_types', 'genome'\n",
       "       uns:\t'dataset_id', 'organism'\n",
       "       obsm:\t'X_pca', 'X_umap'\n",
       "       layers:\t'counts',\n",
       " 'test': MuData object with n_obs × n_vars = 4250 × 129921\n",
       "   var:\t'feature_types'\n",
       "   2 modalities\n",
       "     atac:\t4250 x 116490\n",
       "       obs:\t'nCount_peaks', 'atac_fragments', 'reads_in_peaks_frac', 'blacklist_fraction', 'nucleosome_signal', 'cell_type', 'pseudotime_order_ATAC', 'batch', 'pseudotime_order_GEX', 'is_train'\n",
       "       var:\t'feature_types'\n",
       "       uns:\t'dataset_id', 'gene_activity_var_names', 'organism', 'sample_pm_varnames'\n",
       "       obsm:\t'gene_activity', 'lsi_full', 'lsi_red', 'umap'\n",
       "       layers:\t'counts'\n",
       "     rna:\t4250 x 13431\n",
       "       obs:\t'pct_counts_mt', 'n_counts', 'n_genes', 'size_factors', 'phase', 'cell_type', 'pseudotime_order_GEX', 'batch', 'pseudotime_order_ATAC', 'is_train'\n",
       "       var:\t'gene_ids', 'feature_types', 'genome'\n",
       "       uns:\t'dataset_id', 'organism'\n",
       "       obsm:\t'X_pca', 'X_umap'\n",
       "       layers:\t'counts'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ganoli.GanoliDataset import GanoliMultimodalDataset\n",
    "from muon import MuData\n",
    "\n",
    "class MuDataWithLen(MuData):\n",
    "    \n",
    "    def __len__(self):\n",
    "        try:\n",
    "            return self._len\n",
    "        except:\n",
    "            self._len = min(len(mod) for mod in self.mod.values())\n",
    "            return self._len\n",
    "\n",
    "datasets = {\n",
    "    partition: MuDataWithLen({'atac': atac[partition], 'rna': rna[partition]}) for partition in atac.keys()\n",
    "}\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "749c372b-e2c4-46da-bbea-41e8036700c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from muon import MuData as md\n",
    "from torch.utils.data import DataLoader, BatchSampler, SequentialSampler, RandomSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "samplers = {\n",
    "    'train': RandomSampler,\n",
    "    'val': SequentialSampler,\n",
    "    'test': SequentialSampler\n",
    "}\n",
    "\n",
    "# todo: increase val/test batch size\n",
    "\n",
    "loaders = {\n",
    "    partition: DataLoader(dataset, sampler=BatchSampler(samplers[partition](dataset), batch_size=config['batch_size'], drop_last=False ), collate_fn=lambda x: x[0]) for partition, dataset in datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc8de40-521f-4eb6-b8a0-86af8a097e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3085152101624853"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rna['val'].X.todense().var(axis=0).mean()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83e11da4-4c18-4756-bd70-8a669c38ff90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.349474922340714"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rna['val'].X.todense().var()**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6a7bc05-290a-4e55-a479-613b67763e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote\n"
     ]
    }
   ],
   "source": [
    "LOCAL = False\n",
    "cache_dir=None if LOCAL else \"/om2/user/rogerjin/.cache\"\n",
    "print(\"local\" if LOCAL else \"remote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5948702a-ada2-4cb1-a9e1-88200b970ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del squished\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del rna\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del optimizer\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0c68258-0ce3-4d55-b725-b255ede328e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'max_position_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-efae6d06e833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSquishTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_seq_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# device = 'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'max_position_embeddings'"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "from torch.nn import Linear\n",
    "\n",
    "class SquishTransformer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, output_dim=13431):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased', cache_dir=cache_dir)\n",
    "        self.distilbert.embeddings.word_embeddings = torch.nn.Embedding(116492, 768) # todo: magic numbers\n",
    "        self.pre_classifier = Linear(self.distilbert.config.dim, self.distilbert.config.dim)\n",
    "        self.classifier = Linear(self.distilbert.config.dim, output_dim)\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        out = self.distilbert(**kwargs).last_hidden_state[:, 0] # embedding of cls\n",
    "        out = self.pre_classifier(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "model = SquishTransformer()\n",
    "# device = 'cpu'\n",
    "device = 'cuda:0'\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8337119e-11db-4180-930a-9e74268b476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING INITIAL VALIDATION...\n",
      "val_loss=0.13541 time=0.00016\n",
      "INITIAL VALIDATION COMPLETE\n",
      "\n",
      "STARTING TRAINING...\n",
      "Epoch 1/1000: New best val loss of 0.12815!\n",
      "Saved checkpoint to: /om2/user/rogerjin/checkpoints/test-run/epoch=1-val_loss=0.12815.pt\n",
      "Epoch 1/1000: train_loss=0.14066 best_train_loss=0.14066 best_train_loss_epoch=1 val_loss=0.12815 best_val_loss=0.12815 best_val_loss_epoch=1 time=0.00065\n",
      "TRAINING COMPLETE\n",
      "\n",
      "STARTING TESTING...\n",
      "test_loss=0.13943 time=0.00016\n",
      "TESTING COMPLETE\n"
     ]
    }
   ],
   "source": [
    "from squish_indexing import squish_and_embed\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import AdamW\n",
    "import math\n",
    "from time import perf_counter\n",
    "\n",
    "device = 'cuda:0'\n",
    "loss_fn = MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=config['lr'])\n",
    "\n",
    "def forward_pass(batch, use_binary=config.get('use_binary', False)):\n",
    "    if use_binary:\n",
    "        atac = batch.mod['atac'].X.tocsr().tocoo()\n",
    "    else:\n",
    "        atac = batch.mod['atac'].layers['counts'].tocsr().tocoo()\n",
    "    squished = squish_and_embed(atac, model.distilbert.embeddings.word_embeddings, max_seq_len=config['max_seq_len'])\n",
    "    out = model(inputs_embeds=squished['embeddings'], attention_mask=squished['attention_mask'])\n",
    "    return out\n",
    "    \n",
    "def train_step(batch):\n",
    "    optimizer.zero_grad()\n",
    "    out = forward_pass(batch)\n",
    "    rna = torch.tensor(batch.mod['rna'].X.todense()).float().to(device)\n",
    "    loss = loss_fn(out, rna)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def eval_batch(batch):\n",
    "    with torch.no_grad():\n",
    "        out = forward_pass(batch)\n",
    "        rna = torch.tensor(batch.mod['rna'].X.todense()).float().to(device)\n",
    "        loss = loss_fn(out, rna)\n",
    "    return loss.item()\n",
    "\n",
    "best_checkpoint_path = ''\n",
    "\n",
    "best_train_loss = math.inf\n",
    "best_train_loss_epoch = 0\n",
    "\n",
    "print(f'STARTING INITIAL VALIDATION...')\n",
    "val_loss = 0\n",
    "val_start = perf_counter()\n",
    "for batch in loaders['val']:\n",
    "    val_loss += eval_batch(batch)\n",
    "    break\n",
    "val_end = perf_counter()\n",
    "print(f'val_loss={val_loss:.5f} time={(val_end - val_start)/3600:.5f}')\n",
    "print(f'INITIAL VALIDATION COMPLETE\\n')\n",
    "    \n",
    "best_val_loss = val_loss\n",
    "best_val_loss_epoch = 0\n",
    "    \n",
    "wandb.log({\n",
    "    'epoch': 0,\n",
    "    'val/loss': val_loss,\n",
    "    'val/best_loss': best_val_loss,\n",
    "    'val/time': (val_end - val_start)/3600\n",
    "})\n",
    "\n",
    "print(f'STARTING TRAINING...')\n",
    "for epoch in range(1, config['epochs']+1):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    train_start = perf_counter()\n",
    "    for batch in loaders['train']:\n",
    "        train_loss += train_step(batch)\n",
    "        break\n",
    "    train_end = perf_counter()\n",
    "        \n",
    "    val_start = perf_counter()\n",
    "    for batch in loaders['val']:\n",
    "        val_loss += eval_batch(batch)\n",
    "        break\n",
    "    val_end = perf_counter()\n",
    "        \n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "        best_train_loss_epoch = epoch\n",
    "        \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_loss_epoch = epoch\n",
    "        print(f'Epoch {epoch}/{config[\"epochs\"]}: New best val loss of {val_loss:.5f}!')\n",
    "        \n",
    "        if epoch > 0:\n",
    "            best_checkpoint_path = f\"{wandb.config.checkpoint_dir}/epoch={epoch}-val_loss={val_loss:.5f}.pt\" \n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train/loss': train_loss,\n",
    "                'train/best_loss': best_train_loss,\n",
    "                'train/best_loss_epoch': best_train_loss_epoch,\n",
    "                'val/loss': val_loss,\n",
    "                'val/best_loss': best_val_loss,\n",
    "                'val/best_loss_epoch': best_val_loss_epoch,\n",
    "                }, best_checkpoint_path)\n",
    "\n",
    "            with open(f\"{wandb.config.checkpoint_dir}/best_checkpoint_path.txt\", 'w') as best_f:\n",
    "                best_f.write(best_checkpoint_path)\n",
    "            \n",
    "            print(f'Saved checkpoint to: {best_checkpoint_path}')\n",
    "        \n",
    "    wandb.log({\n",
    "        'epoch': epoch,\n",
    "        'train/loss': train_loss, \n",
    "        'train/best_loss': best_train_loss,\n",
    "        'train/best_loss_epoch': best_train_loss_epoch,\n",
    "        'train/time': (train_end - train_start)/3600,\n",
    "        'val/loss': val_loss,\n",
    "        'val/best_loss': best_val_loss,\n",
    "        'val/best_loss_epoch': best_val_loss_epoch,\n",
    "        'val/time': (val_end - val_start)/3600,\n",
    "        'best_checkpoint_path': best_checkpoint_path,\n",
    "    })\n",
    "    \n",
    "    print(f'Epoch {epoch}/{config[\"epochs\"]}: train_loss={train_loss:.5f} best_train_loss={best_train_loss:.5f} best_train_loss_epoch={best_train_loss_epoch} val_loss={val_loss:.5f} best_val_loss={best_val_loss:.5f} best_val_loss_epoch={best_val_loss_epoch} time={(val_end-train_start)/3600:.5f}')\n",
    "    \n",
    "    break\n",
    "    \n",
    "test_loss = 0\n",
    "test_start = perf_counter()\n",
    "for batch in loaders['test']:\n",
    "    test_loss += eval_batch(batch)\n",
    "    break\n",
    "test_end = perf_counter()\n",
    "    \n",
    "wandb.log({\n",
    "    'epoch': epoch,\n",
    "    'test/loss': test_loss,\n",
    "    'test/time': (test_end-test_start)/3600\n",
    "})\n",
    "print('TRAINING COMPLETE\\n')\n",
    "\n",
    "print('STARTING TESTING...')\n",
    "print(f'test_loss={test_loss:.5f} time={(test_end-test_start)/3600:.5f}')\n",
    "print('TESTING COMPLETE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83d3beef-17ab-4d5e-8435-17a61a3975ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zero(torch.nn.Module):\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66275f32-5be3-4333-aa4c-85ea46d3f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distilbert.embeddings.position_embeddings = Zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4aa74f64-d673-483d-96e8-3f99df6f9f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.7505e-02, -2.5631e-02, -3.6642e-02,  ...,  3.3437e-05,\n",
       "          6.8312e-04,  1.5441e-02],\n",
       "        [ 7.7580e-03,  2.2613e-03, -1.9444e-02,  ...,  2.8910e-02,\n",
       "          2.9753e-02, -5.3247e-03],\n",
       "        [-1.1287e-02, -1.9644e-03, -1.1573e-02,  ...,  1.4908e-02,\n",
       "          1.8741e-02, -7.3140e-03],\n",
       "        ...,\n",
       "        [ 1.7418e-02,  3.4903e-03, -9.5621e-03,  ...,  2.9599e-03,\n",
       "          4.3435e-04, -2.6949e-02],\n",
       "        [ 2.1687e-02, -6.0216e-03,  1.4736e-02,  ..., -5.6118e-03,\n",
       "         -1.2590e-02, -2.8085e-02],\n",
       "        [ 2.6413e-03, -2.3298e-02,  5.4922e-03,  ...,  1.7537e-02,\n",
       "          2.7550e-02, -7.7656e-02]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distilbert.embeddings.position_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e1f1f4-5733-49b9-a7f5-bc38065b1375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/om2/user/rogerjin/conda/ganoli/bin/python'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987f284c-4805-4cec-a497-2eb2617c2327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b9350bb0774a69bb380e2319dafded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837e5edb93e047c09e8ac6386f3fb977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/850M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/long-t5-local-base were not used when initializing LongT5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LongT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongT5Model(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerLocalSelfAttention(\n",
       "            (LocalSelfAttention): LongT5LocalAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): LongT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): LongT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): LongT5LayerSelfAttention(\n",
       "            (SelfAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): LongT5LayerCrossAttention(\n",
       "            (EncDecAttention): LongT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): LongT5LayerFF(\n",
       "            (DenseReluDense): LongT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): LongT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LongT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LongT5Model\n",
    "model = LongT5Model.from_pretrained(\"google/long-t5-local-base\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba9100-63b9-4ecf-b651-702946afc4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ganoli",
   "language": "python",
   "name": "ganoli"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
