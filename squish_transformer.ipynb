{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5569f332-e41d-4d5e-bf30-843f3dd1f909",
   "metadata": {},
   "source": [
    "## Useful links\n",
    "\n",
    "- DistilBertModel [[class]](https://github.com/huggingface/transformers/blob/06a6a4bd516f7d0ba7c4966a2d3d9c0bf07797ae/src/transformers/models/distilbert/modeling_distilbert.py#L459) [[forward]](https://github.com/huggingface/transformers/blob/06a6a4bd516f7d0ba7c4966a2d3d9c0bf07797ae/src/transformers/models/distilbert/modeling_distilbert.py#L538)\n",
    "- [anndata x PyTorch](https://anndata-tutorials.readthedocs.io/en/latest/annloader.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a68a68f-67be-466f-a5fd-40789da4352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c1d54bd-8fc0-4117-a289-60c8c8e0c42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "LOCAL = not torch.cuda.is_available()\n",
    "cache_dir=None if LOCAL else \"/om2/user/rogerjin/.cache\"\n",
    "local_atac_path = '/home/rogerjin/Dropbox/Research/Kellis/masters/data/neurips2021/multiome_atac_processed_training_small.h5ad'\n",
    "remote_atac_path = '/om2/user/rogerjin/data/NeurIPS2021/multiome/multiome_atac_processed_training_small.h5ad'\n",
    "atac_path = local_atac_path if LOCAL else remote_atac_path\n",
    "print(\"local\" if LOCAL else \"remote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c258f54a-9f64-4c64-9050-2dd10e5c870c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel\n",
    "\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased', cache_dir=cache_dir)\n",
    "model.embeddings.word_embeddings = torch.nn.Embedding(116491, 768)\n",
    "\n",
    "device = 'cpu'\n",
    "# device = 'cuda:0'\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b81c9f-ee64-46f5-9554-2edcf7a1f2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2857e+00, -9.5825e-01, -8.6744e-03,  ..., -1.3337e-01,\n",
       "          -8.8122e-01, -9.9369e-01],\n",
       "         [ 1.1567e+00, -1.0683e+01, -1.6499e+01,  ...,  1.2961e+01,\n",
       "           1.1456e+01,  2.2055e+01],\n",
       "         [ 2.2540e+01, -8.0246e+00, -2.9985e+01,  ...,  3.1569e+01,\n",
       "          -1.1161e+02, -3.2046e+01]],\n",
       "\n",
       "        [[ 1.3465e+02,  7.3956e+00,  3.8956e+01,  ..., -1.7088e+02,\n",
       "           1.1801e+01,  7.1604e+01],\n",
       "         [ 4.5243e-01,  6.9270e-01, -1.2919e+00,  ..., -1.8618e+00,\n",
       "          -2.6889e-02, -9.6906e-01],\n",
       "         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from squish_indexing import squish_and_embed\n",
    "\n",
    "atac = torch.tensor([[1, 10, 0, 0, 0, 100], [0, 0, 100, 0, 1, 0]])\n",
    "# manually checked that the embedding looks correct\n",
    "atac_embed = squish_and_embed(atac, model.embeddings.word_embeddings)\n",
    "atac_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "717d1e07-5605-412a-ae7e-db6c912032b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "atac = sc.read_h5ad(atac_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "343576c7-f467-4b53-84c2-4493335555b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnCollectionView object with n_obs × n_vars = 8 × 116490\n",
       "    obsm: 'gene_activity', 'lsi_full', 'lsi_red', 'umap'\n",
       "    layers: 'counts'\n",
       "    obs: 'nCount_peaks', 'atac_fragments', 'reads_in_peaks_frac', 'blacklist_fraction', 'nucleosome_signal', 'cell_type', 'pseudotime_order_ATAC', 'batch', 'pseudotime_order_GEX', 'is_train'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 2. 2. ... 2. 1. 2.]\n",
      "[5 7 3 ... 0 3 0]\n",
      "[    0     0     0 ... 23013 23013 23013]\n",
      "coo\n",
      "[4. 2. 2. ... 2. 1. 2.]\n",
      "(23013,)\n",
      "(23013,)\n"
     ]
    }
   ],
   "source": [
    "from anndata.experimental.pytorch import AnnLoader\n",
    "from squish_indexing import squish_and_embed\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "transform = {\n",
    "    'layers': {\n",
    "        # 'counts': lambda counts_vec: squish_and_embed(torch.tensor(counts_vec), model.embeddings.word_embeddings)\n",
    "        'counts': lambda counts_vec: counts_vec\n",
    "    }\n",
    "}\n",
    "\n",
    "dataloader = AnnLoader(atac, batch_size=8, shuffle=True, convert=transform, use_cuda=False, use_default_converter=False)\n",
    "\n",
    "for batch in dataloader:\n",
    "    display(batch)\n",
    "    print(batch.layers['counts'].data)\n",
    "    print(batch.layers['counts'].indices)\n",
    "    print(batch.layers['counts'].indptr)\n",
    "    print('coo')\n",
    "    coo = batch.layers['counts'].tocsr().tocoo()\n",
    "    print(batch.layers['counts'].tocoo().data)\n",
    "    print(batch.layers['counts'].tocoo().row.shape)\n",
    "    print(batch.layers['counts'].tocoo().col.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "83782e80-02f9-4fa3-b575-a389b2e0e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functorch\n",
    "\n",
    "DEFAULT_ARANGE_LEN = 10\n",
    "INDEX_PAD_TOKEN = 116490\n",
    "COUNT_PAD_TOKEN = 0\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "def cyclic_arange(lengths, arange=torch.arange(DEFAULT_ARANGE_LEN)):\n",
    "    max_len = lengths.max().item()\n",
    "    if max_len > DEFAULT_ARANGE_LEN:\n",
    "        arange = torch.arange(max_len)\n",
    "    arange_vmap = functorch.vmap(lambda length: (arange + 1) * (arange < length))\n",
    "    vmapped = arange_vmap(lengths).flatten()\n",
    "    return vmapped[vmapped.nonzero()].flatten() - 1\n",
    "\n",
    "def index_and_pad(indices, data, pad_token):\n",
    "    return torch.sparse_coo_tensor(indices, data - pad_token).to_dense() + pad_token\n",
    "\n",
    "def squish_and_embed(batch_coo, embedding):\n",
    "    rows = torch.tensor(batch_coo.row)\n",
    "    _, num_nonzeros = torch.unique_consecutive(rows, return_counts=True)\n",
    "    assert _.shape[0] == BATCH_SIZE # otherwise there's an all-0 sequence in the batch\n",
    "    max_seq_len = num_nonzeros.max().item()\n",
    "    sparse_indices = torch.stack([rows, cyclic_arange(num_nonzeros)])\n",
    "    squish_indices = index_and_pad(sparse_indices, batch_coo.col, INDEX_PAD_TOKEN)\n",
    "    counts = index_and_pad(sparse_indices, batch_coo.data, COUNT_PAD_TOKEN)\n",
    "    display(squish_indices)\n",
    "    display(counts)\n",
    "    return {\n",
    "        'indices': squish_indices,\n",
    "        'counts': counts,\n",
    "        'squish_embeddings': embedding(squish_indices) * counts.unsqueeze(-1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0c3b2b01-e309-432a-a444-6954542811be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4118071/68443200.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indices = torch.stack([torch.tensor(coo.row), cyclic_arange(torch.tensor(counts))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ...,    7,    7,    7],\n",
       "        [   0,    1,    2,  ..., 3215, 3216, 3217]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3596, 2013, 1309, 2753, 3675, 5674,  775, 3218])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5674"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "        [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "        [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 4., 18.,  2.,  ...,  2.,  2.,  2.],\n",
       "        [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "        [ 2.,  4.,  4.,  ...,  0.,  0.,  0.]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[     6,     21,     43,  ...,      0,      0,      0],\n",
       "        [     6,     46,     52,  ...,      0,      0,      0],\n",
       "        [   117,    191,    196,  ...,      0,      0,      0],\n",
       "        ...,\n",
       "        [     5,      6,     21,  ..., 116446, 116448, 116457],\n",
       "        [     6,     10,     72,  ...,      0,      0,      0],\n",
       "        [     5,      6,     86,  ...,      0,      0,      0]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output, counts = torch.unique_consecutive(torch.tensor(coo.row), return_counts=True)\n",
    "PAD_TOKEN = 0\n",
    "seq_len = counts.max().item()\n",
    "num_rows = output.shape[0]\n",
    "indices = torch.stack([torch.tensor(coo.row), cyclic_arange(torch.tensor(counts))])\n",
    "display(indices)\n",
    "count_tensor = torch.sparse_coo_tensor(indices, coo.data - PAD_TOKEN).to_dense() + PAD_TOKEN\n",
    "index_tensor = torch.sparse_coo_tensor(indices, coo.col - PAD_TOKEN).to_dense() + PAD_TOKEN\n",
    "display(output)\n",
    "display(counts)\n",
    "display(seq_len)\n",
    "display(count_tensor)\n",
    "display(index_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bd72ce70-b080-4d7d-9c15-7089cb864146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     6,     21,     43,  ..., 116490, 116490, 116490],\n",
       "        [     6,     46,     52,  ..., 116490, 116490, 116490],\n",
       "        [   117,    191,    196,  ..., 116490, 116490, 116490],\n",
       "        ...,\n",
       "        [     5,      6,     21,  ..., 116446, 116448, 116457],\n",
       "        [     6,     10,     72,  ..., 116490, 116490, 116490],\n",
       "        [     5,      6,     86,  ..., 116490, 116490, 116490]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "        [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "        [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 4., 18.,  2.,  ...,  2.,  2.,  2.],\n",
       "        [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "        [ 2.,  4.,  4.,  ...,  0.,  0.,  0.]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'indices': tensor([[     6,     21,     43,  ..., 116490, 116490, 116490],\n",
       "         [     6,     46,     52,  ..., 116490, 116490, 116490],\n",
       "         [   117,    191,    196,  ..., 116490, 116490, 116490],\n",
       "         ...,\n",
       "         [     5,      6,     21,  ..., 116446, 116448, 116457],\n",
       "         [     6,     10,     72,  ..., 116490, 116490, 116490],\n",
       "         [     5,      6,     86,  ..., 116490, 116490, 116490]],\n",
       "        dtype=torch.int32),\n",
       " 'counts': tensor([[ 6.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "         [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "         [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 4., 18.,  2.,  ...,  2.,  2.,  2.],\n",
       "         [ 2.,  2.,  2.,  ...,  0.,  0.,  0.],\n",
       "         [ 2.,  4.,  4.,  ...,  0.,  0.,  0.]], dtype=torch.float64),\n",
       " 'squish_embeddings': tensor([[[ 2.8917e+00, -8.3627e+00, -3.2458e+00,  ..., -3.7075e+00,\n",
       "           -8.5471e+00, -3.6615e+00],\n",
       "          [-3.3926e+00,  7.1447e-01,  2.6079e+00,  ..., -2.4539e-01,\n",
       "           -2.2277e+00, -9.1005e-01],\n",
       "          [ 2.0121e+00,  2.0304e+00,  4.2971e+00,  ...,  5.4122e-01,\n",
       "           -9.3851e-01,  4.4096e+00],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]],\n",
       " \n",
       "         [[ 9.6389e-01, -2.7876e+00, -1.0819e+00,  ..., -1.2358e+00,\n",
       "           -2.8490e+00, -1.2205e+00],\n",
       "          [ 4.9161e+00,  1.2106e+00, -4.3678e-01,  ..., -9.9173e-01,\n",
       "            5.3654e-01,  1.1409e+00],\n",
       "          [-2.0008e+00,  2.2510e+00, -1.0195e+00,  ..., -5.4534e-01,\n",
       "           -4.7405e+00,  1.4598e+00],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]],\n",
       " \n",
       "         [[ 2.0967e+00, -5.7616e-01, -1.8250e+00,  ...,  1.9227e+00,\n",
       "           -7.1275e-04,  6.5589e-01],\n",
       "          [-1.7491e+00, -5.1442e+00,  2.2415e+00,  ...,  2.2173e+00,\n",
       "            7.1585e-01, -1.3320e+00],\n",
       "          [-2.5399e+00,  7.7213e-02, -9.3066e-01,  ...,  2.4399e+00,\n",
       "           -6.3507e-01,  3.1590e+00],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 9.6097e-01, -8.5291e-02,  2.5098e+00,  ...,  1.2143e+00,\n",
       "           -6.6500e+00,  3.2395e+00],\n",
       "          [ 8.6750e+00, -2.5088e+01, -9.7375e+00,  ..., -1.1123e+01,\n",
       "           -2.5641e+01, -1.0984e+01],\n",
       "          [-3.3926e+00,  7.1447e-01,  2.6079e+00,  ..., -2.4539e-01,\n",
       "           -2.2277e+00, -9.1005e-01],\n",
       "          ...,\n",
       "          [ 6.0047e+00,  4.1739e-01, -2.0908e+00,  ..., -2.4165e+00,\n",
       "            3.2712e+00,  1.0558e+00],\n",
       "          [ 1.4332e+00,  1.0380e+00,  1.0467e+00,  ..., -1.0219e+00,\n",
       "           -1.5436e+00, -1.8646e-01],\n",
       "          [ 1.3592e+00, -2.8072e-01, -5.7813e-03,  ..., -5.5449e+00,\n",
       "           -3.0501e+00,  2.3302e+00]],\n",
       " \n",
       "         [[ 9.6389e-01, -2.7876e+00, -1.0819e+00,  ..., -1.2358e+00,\n",
       "           -2.8490e+00, -1.2205e+00],\n",
       "          [-1.1179e+00, -4.2179e-02, -3.0222e-01,  ..., -1.1290e+00,\n",
       "            6.8071e-01, -3.9563e+00],\n",
       "          [ 6.4549e-01, -5.4349e-01,  7.7998e-01,  ...,  2.3576e-01,\n",
       "            1.6561e+00,  5.2868e-02],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]],\n",
       " \n",
       "         [[ 4.8048e-01, -4.2646e-02,  1.2549e+00,  ...,  6.0717e-01,\n",
       "           -3.3250e+00,  1.6197e+00],\n",
       "          [ 1.9278e+00, -5.5751e+00, -2.1639e+00,  ..., -2.4717e+00,\n",
       "           -5.6981e+00, -2.4410e+00],\n",
       "          [ 1.7871e-01, -4.2879e+00,  8.9208e+00,  ...,  1.6221e+00,\n",
       "            4.6348e+00,  2.5364e+00],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]]], dtype=torch.float64,\n",
       "        grad_fn=<MulBackward0>)}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squish_and_embed(coo, model.embeddings.word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58db26-f8c8-4456-829a-830e87ca521f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ganoli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
